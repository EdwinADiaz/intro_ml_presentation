{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>K-Nearest Neighbors</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Known as the \"easy\" machine learning model\n",
    "    - Classifies an event based on its closest relatives in the data the model has been trained on. Hence the term \"Nearest Neighbors\". K = number of neighbors.\n",
    "    - Known as a voting classifier because n neighbors vote for the classification.\n",
    "    - Uses Euclidean Distance to calculate similarity.\n",
    "    - Pros: Fast, intuitive, easy to interpret, ability to make probabilities.\n",
    "    - Cons: Poor at handling many features, especially \"noisy\" features because it treats every feature equally. Not good with small sample sizes. Usually requires scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"EuclideanDistanceGraphic.jpg\")\n",
    "#Source: Analytics Vidya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make fake data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = make_classification(n_samples=200,\n",
    "                           n_features=2,\n",
    "                           n_classes=2,\n",
    "                           n_informative=2,\n",
    "                           n_redundant=0,\n",
    "                            class_sep=.75,\n",
    "                           random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[0]\n",
    "y = data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,11))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=250)\n",
    "plt.xlabel(\"Feature One\")\n",
    "plt.ylabel(\"Feature Two\")\n",
    "plt.xlim(-3,3)\n",
    "plt.ylim(-3,3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a KNN model using 3 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize model and set n_neighbors equal to 3\n",
    "knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "#Fit the model on the \"fake data\"\n",
    "knn3.fit(X,y)\n",
    "#Find the accuracy score of the model on the data\n",
    "score3 = float(knn3.score(X,y))\n",
    "print \"The model accurately labelled {:.2f} percent of the data\".format(score3*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with 5 neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intialize model and set n_neighbors equal to 5\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "#Fit the model on the \"fake data\"\n",
    "knn5.fit(X,y)\n",
    "#Find the accuracy score of the model on the data\n",
    "score5 = float(knn5.score(X,y))\n",
    "print \"The model accurately labelled {:.2f} percent of the data\".format(score5*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model on a new point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = np.asarray([0.18,0.15]).reshape(1,-1)\n",
    "pred3 = knn3.predict(new_data)\n",
    "pred5 = knn5.predict(new_data)\n",
    "print \"The knn3 model thinks new_data belongs to class {}\".format(pred3[0])\n",
    "print \"The knn5 model thinks new_data belongs to class {}\".format(pred5[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize new point in relation to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,11))\n",
    "plt.xlim(0,0.4)\n",
    "plt.ylim(0,.35)\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=250)\n",
    "#Plot of new_data point\n",
    "plt.scatter([0.18], [0.15], c=\"purple\",marker=\"*\", s= 1500)\n",
    "plt.xlabel(\"Feature One\")\n",
    "plt.ylabel(\"Feature Two\")\n",
    ";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the data but can we see the model itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, n_neighbors):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y,s=40, alpha=0.4)\n",
    "    plt.title(\"Plot of {} neighbors\".format(n_neighbors))\n",
    "    plt.xlabel(\"Feature One\")\n",
    "    plt.ylabel(\"Feature Two\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the knn3 model\n",
    "plot_decision_boundary(knn3, X, y, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the knn3 model\n",
    "plot_decision_boundary(knn5, X, y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 7 neighbors\n",
    "knn13 = KNeighborsClassifier(n_neighbors=13)\n",
    "knn13.fit(X,y)\n",
    "plot_decision_boundary(knn13, X, y, 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training vs testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    - Training data is the data used to create the model. The data that the machine learns from.\n",
    "    - Testing data is the data used to evaluate the ability of the model. \n",
    "    - Any model can be used to classify data points it has seen, but the best models accurately classify foreign data.\n",
    "    - The point of a machine learning model is keep it so you can repeatedly use it on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(\"train_test.jpg\")\n",
    "#Source: Nizam Muhammad Pega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Spotify data\n",
    "A dataset of songs I like and dislike and their attributes from Spotify. 1 = like, 0 = dislike<br><br>\n",
    "\n",
    "\n",
    "\n",
    "<b>Attributes:</b>\n",
    "\n",
    "        Acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n",
    "        \n",
    "        Danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n",
    "        \n",
    "        Instrumentalness: Predicts whether a track contains no vocals. \"Ooh\" and \"aah\" sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly \"vocal\". The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.\n",
    "        \n",
    "        Valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n",
    "        \n",
    "        Energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n",
    "        \n",
    "More details: https://developer.spotify.com/web-api/get-audio-features/\n",
    "\n",
    "My article using this dataset: https://opendatascience.com/blog/a-machine-learning-deep-dive-into-my-spotify-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in and view subset of data\n",
    "df = pd.read_pickle(\"songs_ML.pkl\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train KNN on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"target\",axis =1)\n",
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the null accuracy aka the benchmark score\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X,y)\n",
    "score = float(knn.score(X,y))\n",
    "print \"The model accurately labelled {:.2f} percent of the data\".format(score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps:\n",
    "\n",
    "1. Split data into training and testing sets.\n",
    "2. Fit knn model with 5 neighbors on the training data.\n",
    "3. Make predictions on the testing data with the trained model.\n",
    "4. Compare predicted labels of the testing data to its actual labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)\n",
    "#Step 2:\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "#Step 3:\n",
    "predictions = knn.predict(X_test)\n",
    "#Step 4:\n",
    "testing_score = accuracy_score(y_test, predictions)\n",
    "# knn.score(X_test, y_test) is another way to get the accuracy score\n",
    "print \"The model accurately classified {:.2f} of the testing data\".format(testing_score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noticeable drop in the score, how come?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model scores for testing and training data for a range of neighbor values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "for i in range(3,25,2):\n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "    model.fit(X_train, y_train)\n",
    "    trs = model.score(X_train, y_train)\n",
    "    tes = model.score(X_test, y_test)\n",
    "    train_scores.append(trs)\n",
    "    test_scores.append(tes)\n",
    "    \n",
    "plt.figure(figsize=(18,14))    \n",
    "plt.plot(range(3,25,2), train_scores, label= \"Training Scores\",linewidth= 11)\n",
    "plt.plot(range(3,25,2), test_scores, label= \"Testing Scores\",linewidth= 11)\n",
    "plt.xlabel(\"N-Neighbors\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
